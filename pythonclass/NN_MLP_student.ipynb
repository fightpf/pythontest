{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXd84-Otthwp"
   },
   "source": [
    "# 手刻神經網絡 with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<!---\n",
    "也許你會覺得疑惑<br>\n",
    "為什麼我們要花這麼多時間來學神經網路<br>\n",
    "它真的有這麼厲害嗎?<br>\n",
    "我這邊可以清楚的告訴你<br>\n",
    "有<br>\n",
    "但是你可能不會相信我的話<br>\n",
    "你可以相信Universal Approximate Theorem<br>\n",
    "這個理論形容如果我們有夠大的model的話<br>\n",
    "我們可以\n",
    "According to , Neural Networks can approximate as well as learn and represent any function given a large enough layer and desired error margin.\n",
    "--->\n",
    "我們這邊只會使用numpy <br>\n",
    "\n",
    "numpy 是一個支援很多數學的package <br>\n",
    "不過我們這邊只會使用五個function<br>\n",
    "numpy.random.seed(number) → 控制output 結果<br>\n",
    "numpy.random.random((size=None)) → 建立一個數值為0到1之間的隨機array<br>\n",
    "numpy.zeros((shape)) → 建立一個全部為0的矩陣<br>\n",
    "numpy.matmul(matrix1, matrix2) → 矩陣相乘<br>\n",
    "numpy.vstack(matrix1, matrix2) → 已列的方式對array或matrix做組合<br>\n",
    "如果想用聽的以下附上影片<br>\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/65pIPhVx4l0/0.jpg)](http://www.youtube.com/watch?v=65pIPhVx4l0)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kkMISk-1thwr"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9Y81C8tthwr"
   },
   "source": [
    "# 1. Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIWjQc4ethwu"
   },
   "source": [
    "<hr>\n",
    "在這個章節我們要來嘗試如何利用程式的方式來呈現微分與偏微分<br>\n",
    "所以如果你大學的微積分都已經完整的歸還給老師的話請自己先自行懺悔<br><br>\n",
    "(懺悔時間......)<br><br>\n",
    "如果你已經懺悔完的話<br>\n",
    "那就讓我們，繼續看下去<br><br>\n",
    "\n",
    "這邊想特別聲明一下<br>\n",
    "以下教各位的程式並不是最optimized 方法<br>\n",
    "只是為了大家方便理解<br>\n",
    "用單純數學的概念來做<br>\n",
    "如果想用聽的以下附上影片<br>\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/RKYEUORyQtA/0.jpg)](http://www.youtube.com/watch?v=RKYEUORyQtA)\n",
    "<hr>\n",
    "\n",
    "##  (1)先寫出一個簡單的方程式：  $ f(x) = 0.05x^2 + 0.8x $\n",
    "> 提示：Python 的乘法是 * ，指數是 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A8VAsvXVthwv"
   },
   "outputs": [],
   "source": [
    "# Define our target function\n",
    "def my_function(x):\n",
    "    # Write your code below!\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxYYwJSzthwy"
   },
   "source": [
    "## (2)定義微分公式：$ \\frac{df(x)}{dx} =\\lim\\limits_{h\\to 0} \\frac{(f(x+h) - f(x))}{h} $\n",
    "> 提示：不要將 $ epsilon $設的太大或是太小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jBFAe5b3thwz"
   },
   "outputs": [],
   "source": [
    "# Define derivative\n",
    "def derivative(f, x):\n",
    "    # f in here stands for our function, and x is our input\n",
    "    # I usually set a variable called epsilon, it represents a small number\n",
    "    # Write your code below!\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwWfr0LHthw2"
   },
   "source": [
    "## (3)用我們的公式算 $x=5$ 和 $x=10$ 的微分\n",
    ">可以嘗試改變epsilon來觀察微分的變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Fv0l_4mpthw3",
    "outputId": "bb419b91-a8c1-4355-828d-6de24c860d81"
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-50\n",
    "print('The derivative of x at 5 is',derivative(my_function, 5))\n",
    "print('The derivative of x at 10 is',derivative(my_function, 10))\n",
    "\n",
    "epsilon = 1e-4\n",
    "print('The derivative of x at 5 is',derivative(my_function, 5))\n",
    "print('The derivative of x at 10 is',derivative(my_function, 10))\n",
    "\n",
    "epsilon = 1\n",
    "print('The derivative of x at 5 is',derivative(my_function, 5))\n",
    "print('The derivative of x at 10 is',derivative(my_function, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)微分結論\n",
    "    從以上的結果還有數學的推論，我們得知如果我們有一個算式 \n",
    "> ### $f(X)=X^n$ \n",
    "> ### $\\frac{df(X)}{dX} = nX^{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Partial Derivative \n",
    "\n",
    "\n",
    "##  (1)先寫出一個簡單的方程式：  $ f(x) = 2x^2 + 3xy + 5y^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function2(X):\n",
    "    # For me, I tend to use uppercase X to represent a list or a matrix\n",
    "    # and use lowercase x to represent a single value\n",
    "    # You can change the variable to whatever that you feel natural \n",
    "    # just remember that the X here represent a list of two variables x and y\n",
    "    # in which X[0] represents x and X[1] represents y\n",
    "    # Write your code below!\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)定義偏微分公式：$ \\frac{df(x_i)}{dx_a} =\\lim\\limits_{h\\to 0} \\frac{f(x_1,\\,\\cdots\\,x_a\\,+\\,h,\\,x_{a+1},\\,\\cdots\\,x_n)\\,-\\,f(x_1\\,\\cdots\\,x_n)}{h} $\n",
    ">提示：可以嘗試建立一個新的array, 然後把xlist裡面第i個位置的值update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "def partial_derivative(f, X, i):\n",
    "    # f is our function, and i is simply the index which we are \n",
    "    # excuting our partial derivative on \n",
    "    \n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3)用我們的公式算 $f_x(2,\\,3)$ 和 $f_y(2,\\,3)$ 的微分\n",
    ">可以重新定義一個function來看partial對於你自己寫的function的變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Bq1nqQNzthw7",
    "outputId": "cd1d27b8-5397-434a-a05a-dd350094c5b5"
   },
   "outputs": [],
   "source": [
    "print('The partial with respect to x at (2, 3) is',partial_derivative(my_function2, np.array([2., 3.]), 0))\n",
    "print('The partial with respect to y at (2, 3) is',partial_derivative(my_function2, np.array([2., 3.]), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)偏微分結論\n",
    "    從以上的結果還有數學的推論，我們得知如果我們有一個算式 \n",
    "> ### $f(X,\\,Y)=X^n\\,+\\,X^nY^n\\,+\\,Y^n$<br>\n",
    "> ### $f_X(X,\\,Y) = nX^{n-1}\\,+\\,nX^{n-1}Y^n$\n",
    "> ### $f_Y(X,\\,Y) = nX^nY^{n-1}\\,+\\,nY^{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient\n",
    "<hr>\n",
    "也許你會覺得 $gradient$ 的概念有點抽象，但其實他的概念很簡單，$gradient$ 就是把每一個偏微分湊在一起而已。<br>\n",
    "用一個比較現實的例子來看，假設一個山谷的是一個由x和y組成的function，<br>\n",
    "Meaning $ H(x, y)=height$ 。而 $ H $ 的 $Gradient$ 即是山谷的坡度。<br>\n",
    "而我們今天的目的就是用坡度來尋找我們的山谷谷底 ($gradient\\,descent$)<br>\n",
    "如下圖<br>\n",
    "\n",
    "\n",
    "![alt text](https://blog.paperspace.com/content/images/2018/05/68747470733a2f2f707669676965722e6769746875622e696f2f6d656469612f696d672f70617274312f6772616469656e745f64657363656e742e676966.gif)<br>\n",
    "[image source](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)\n",
    "<br>\n",
    "要達到 $gradient\\,descent$ 之前還有一些東西要先定義<br>\n",
    "接下來讓我們自己手寫看看 $gradient$ 的程式，以下提供 $gradient$ 的算式\n",
    "### $ \\nabla f({x_1},{x_2},{x_3},\\,\\ldots) = \\frac{\\partial f}{\\partial {x_1}}i+ \\frac{\\partial f}{\\partial {x_2}}j+ \\frac{\\partial f}{\\partial {x_3}}k \\cdots$\n",
    "\n",
    "如果想用聽的以下附上影片<br>\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/6NKIl2gKJOw/0.jpg)](http://www.youtube.com/watch?v=6NKIl2gKJOw)\n",
    "<hr>\n",
    "\n",
    "> 提示：可以使用 $for\\,loop$ 來寫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f, X):\n",
    "    # You can use the partial_derivative function you wrote above here\n",
    "    # Write your code below!\n",
    "    \n",
    "    \n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgqOUOP7thxc"
   },
   "source": [
    "# 4. Loss \n",
    "<hr>\n",
    "由於中文的對於 $loss$ 的定義看起來像文言文一樣，所以我這邊附上英文的定義<br>\n",
    "\n",
    ">Definition: a loss function or cost function is a function that maps an event or values <br> \n",
    ">of one or more variables onto a real number intuitively representing some \"cost\" <br>\n",
    ">associated with the event. <br>\n",
    "\n",
    "而我們要做的事情就是將 $loss$ 最小化<br>\n",
    "$loss\\,function$ 非常多種，這裡會來教大家寫一個常用的$loss:\\,Mean\\,Square\\,Error\\,(MSE)$ <br>\n",
    "<br>\n",
    "$MSE$ 的概念很簡單，只需要將實際值減掉預測值之後平方加總然後除以總數就好<br>\n",
    "如果覺得很饒舌的話就直接看算式就好<br>\n",
    "### $MSE\\,=\\,\\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} (Y_i-\\hat{Y_i})^2$\n",
    "那我們就來練習寫一下這個簡單的 $MSE$ 吧！\n",
    "如果想用聽的以下附上影片<br>\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/FbaWjcEZ844/0.jpg)](http://www.youtube.com/watch?v=FbaWjcEZ844)\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(true, pred):\n",
    "    # Do remember MSE use true - predict value\n",
    "    # reversing the order may result in strange output\n",
    "    # Write your code below!\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Building\n",
    "<hr>\n",
    "終於到了建model 的時候了！<br>\n",
    "不過我們這邊要偷偷 import 一個 sklearn 的 Package<br>\n",
    "sklearn裡面有一些整理很乾淨的資料<br>\n",
    "而由於我們這次的重點是手寫模型<br>\n",
    "我們這邊就會直接拿他們的 Toy Dateset 裡面的 Iris 來做示範<br>\n",
    "如果想用聽的以下附上影片<br>\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/ZDQ8L_d3sYI/0.jpg)](http://www.youtube.com/watch?v=ZDQ8L_d3sYI)\n",
    "<hr>\n",
    "\n",
    ">特別注意<br>\n",
    ">在現實中的case因為資料不會這麼整齊乾淨漂亮<br>\n",
    ">所以這邊的切分法只有在這組資料可以work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "names = iris.target_names\n",
    "\n",
    "# Train valid test split\n",
    "\n",
    "X_train = np.vstack([X[0:40], X[50:90], X[100:140]])\n",
    "X_valid = np.vstack([X[40:45], X[90:95], X[140:145]])\n",
    "X_test = np.vstack([X[45:50], X[95:100], X[145:150]])\n",
    "\n",
    "Y_train = np.vstack([Y[0:40], Y[50:90], Y[100:140]])\n",
    "Y_valid = np.vstack([Y[40:45], Y[90:95], Y[140:145]])\n",
    "Y_test = np.vstack([Y[45:50], Y[95:100], Y[145:150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1)定義function\n",
    "<hr>\n",
    "我們這邊要寫的架構很簡單<br>\n",
    "將我們所有的資料X乘上我們的Weight W (我們這邊先不管Bias)<br>\n",
    "然後我們使用的是MSE<br>\n",
    "所以我們要套用MSE的算式如以下<br>\n",
    "\n",
    "### $f\\,=\\,\\frac{1}{2n} \\displaystyle\\sum_{i=1}^{n} (Y_i-XW)^2$\n",
    "\n",
    "眼尖的各位一定會發現<br>\n",
    "我們這個算式是除以2n而不是n<br>\n",
    "2在這邊的目的只是單純為了不要讓資料的數量影響訓練的過程<br>\n",
    "事實上放其他的數字也是可以<br>\n",
    "只不過放2的話在微分的過程或比較漂亮<br>\n",
    "所以這邊才放2<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(W, data=X_train, target=Y_train):\n",
    "    # Do notice how we write data=X_train\n",
    "    # This kinda method is not usually ideal\n",
    "    # but in here this will come in handy\n",
    "    # Write your code below!\n",
    "    \n",
    "    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)Gradient Descent\n",
    "<hr>\n",
    "我們接下來要來寫梯度下降<br>\n",
    "有了上面的式子以後<br>\n",
    "我們只要在每一個 loop 裡面跑一次我們的 gradient<br>\n",
    "然後將我們的 Weight 減掉 alpha 乘上 gradient<br>\n",
    "一直 update 之後就可以得到我們要的答案了<br>\n",
    "<br>\n",
    "有了 update 過後的 Weight 我們就可以拿來進行預測了<br>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, Y_train, X_valid, Y_valid, W, alpha, num_iters):\n",
    "    # This is the hardest part of this code\n",
    "    # Do notice that you can choose to use the gradient we just wrote\n",
    "    # if you do not want to use gradient then you can write this code \n",
    "    # without the function we define above\n",
    "    # Write your code below!\n",
    "    m = len(Y_train)\n",
    "    train_loss = np.zeros((num_iters, 1))\n",
    "    valid_loss = np.zeros((num_iters, 1))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return W, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "np.random.seed(37)\n",
    "W = np.random.random((4,1))\n",
    "W, train_loss, valid_loss = gradient_descent(X_train, Y_train, X_valid, Y_valid, W, 0.03, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.matmul(X_test, W).round()\n",
    "print('The MSE score of our prediction is ', MSE(Y_test, predict)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "01_DeepLearning_without_framework_numeric_method.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
